{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BIL635.HW.2_v0.4",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2uvlQDuu2fDzQoyWioxYd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thaspolat/autonomic.computing/blob/master/BIL635_HW_2_v0_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xoxYmIwRJAw"
      },
      "source": [
        "#Vanilla RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEp4hGer1qWx",
        "outputId": "4433ee37-f24d-4a55-ffba-6678f187a457"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "textFileName = '/content/gdrive/My Drive/Colab Notebooks/Baris_Manco_lyricsText.txt'\r\n",
        "textFile = '/content/gdrive/My Drive/Colab Notebooks/input.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skipl0qrH7qR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c03e3d-29b2-4937-de14-d0f751e7b3ca"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "# data I/O\r\n",
        "data = open(textFileName, 'r').read()  # should be simple plain text file\r\n",
        "chars = list(set(data))\r\n",
        "data_size, vocab_size = len(data), len(chars)\r\n",
        "print('data has %d characters, %d unique.' % (data_size, vocab_size))\r\n",
        "char_to_ix = {ch: i for i, ch in enumerate(chars)}\r\n",
        "ix_to_char = {i: ch for i, ch in enumerate(chars)}\r\n",
        "\r\n",
        "# hyperparameters\r\n",
        "hidden_size = 20  # size of hidden layer of neurons\r\n",
        "seq_length = 3  # number of steps to unroll the RNN for\r\n",
        "learning_rate = 1e-1\r\n",
        "\r\n",
        "# model parameters\r\n",
        "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # input to hidden\r\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden to hidden\r\n",
        "Why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden to output\r\n",
        "bh = np.zeros((hidden_size, 1))  # hidden bias\r\n",
        "by = np.zeros((vocab_size, 1))  # output bias\r\n",
        "\r\n",
        "\r\n",
        "def lossFun(inputs, targets, hprev):\r\n",
        "    \"\"\"\r\n",
        "    inputs,targets are both list of integers.\r\n",
        "    hprev is Hx1 array of initial hidden state\r\n",
        "    returns the loss, gradients on model parameters, and last hidden state\r\n",
        "    \"\"\"\r\n",
        "    xs, hs, ys, ps = {}, {}, {}, {}\r\n",
        "    hs[-1] = np.copy(hprev)\r\n",
        "    loss = 0\r\n",
        "    # forward pass\r\n",
        "    for t in range(len(inputs)):\r\n",
        "        xs[t] = np.zeros((vocab_size, 1))  # encode in 1-of-k representation\r\n",
        "        xs[t][inputs[t]] = 1\r\n",
        "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + bh)  # hidden state\r\n",
        "        ys[t] = np.dot(Why, hs[t]) + by  # unnormalized log probabilities for next chars\r\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # probabilities for next chars\r\n",
        "        loss += -np.log(ps[t][targets[t], 0])  # softmax (cross-entropy loss)\r\n",
        "    # backward pass: compute gradients going backwards\r\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\r\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\r\n",
        "    dhnext = np.zeros_like(hs[0])\r\n",
        "    for t in reversed(range(len(inputs))):\r\n",
        "        dy = np.copy(ps[t])\r\n",
        "        dy[targets[\r\n",
        "            t]] -= 1  # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\r\n",
        "        dWhy += np.dot(dy, hs[t].T)\r\n",
        "        dby += dy\r\n",
        "        dh = np.dot(Why.T, dy) + dhnext  # backprop into h\r\n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh  # backprop through tanh nonlinearity\r\n",
        "        dbh += dhraw\r\n",
        "        dWxh += np.dot(dhraw, xs[t].T)\r\n",
        "        dWhh += np.dot(dhraw, hs[t - 1].T)\r\n",
        "        dhnext = np.dot(Whh.T, dhraw)\r\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\r\n",
        "        np.clip(dparam, -5, 5, out=dparam)  # clip to mitigate exploding gradients\r\n",
        "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs) - 1]\r\n",
        "\r\n",
        "\r\n",
        "def sample(h, seed_ix, n):\r\n",
        "    \"\"\"\r\n",
        "    sample a sequence of integers from the model\r\n",
        "    h is memory state, seed_ix is seed letter for first time step\r\n",
        "    \"\"\"\r\n",
        "    x = np.zeros((vocab_size, 1))\r\n",
        "    x[seed_ix] = 1\r\n",
        "    ixes = []\r\n",
        "    for t in range(n):\r\n",
        "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\r\n",
        "        y = np.dot(Why, h) + by\r\n",
        "        p = np.exp(y) / np.sum(np.exp(y))\r\n",
        "        ix = np.random.choice(range(vocab_size), p=p.ravel())\r\n",
        "        x = np.zeros((vocab_size, 1))\r\n",
        "        x[ix] = 1\r\n",
        "        ixes.append(ix)\r\n",
        "    return ixes\r\n",
        "\r\n",
        "# n=iteration counter\r\n",
        "n, p = 0, 0\r\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\r\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by)  # memory variables for Adagrad\r\n",
        "smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # loss at iteration 0\r\n",
        "while n <= 5000:  # run loop for 5000 iterations\r\n",
        "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\r\n",
        "    if p + seq_length + 1 >= len(data) or n == 0:\r\n",
        "        hprev = np.zeros((hidden_size, 1))  # reset RNN memory\r\n",
        "        p = 0  # go from start of data\r\n",
        "    inputs = [char_to_ix[ch] for ch in data[p:p + seq_length]]\r\n",
        "    targets = [char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]]\r\n",
        "\r\n",
        "    # sample from the model now and then\r\n",
        "    if n % 100 == 0:\r\n",
        "        sample_ix = sample(hprev, inputs[0], 200)\r\n",
        "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\r\n",
        "        print ('----\\n %s \\n----' % (txt,))\r\n",
        "\r\n",
        "    # forward seq_length characters through the net and fetch gradient\r\n",
        "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\r\n",
        "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\r\n",
        "    if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss))  # print progress\r\n",
        "\r\n",
        "    # perform parameter update with Adagrad\r\n",
        "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\r\n",
        "                                  [dWxh, dWhh, dWhy, dbh, dby],\r\n",
        "                                  [mWxh, mWhh, mWhy, mbh, mby]):\r\n",
        "        mem += dparam * dparam\r\n",
        "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\r\n",
        "\r\n",
        "    p += seq_length  # move data pointer\r\n",
        "    n += 1  # iteration counter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 207287 characters, 81 unique.\n",
            "----\n",
            " ğÇ'$cçCÜLOT51brç:şuYâZG8‘JgN0Bav2Kü2!ç2 vSoS?fYMybg5öl…‘A9oŞkÖ‘h12İ1bf:'wÜu237.âıEfEuLpZâŞHdzOJsRjZYg8OöAiFrEeöc‘FğkÖ7Y?r1N0ıjC-:fyÇhTşLS66w!.…Ti'nR502'y…$VYc:HKL,5tLüD5â‘'VLuwrınGZvZmİvkOY…ÜL‘şT:HN3s \n",
            "----\n",
            "iter 0, loss: 13.183348\n",
            "----\n",
            " oSema ansz eo cr\n",
            "do iZoac -ytrlatöigkmvd Dg \n",
            "6lymo nlsAmas\n",
            "DigBa ŞunuueidroioDeÜeuk\n",
            "b6lDynüe\n",
            "şk \n",
            "8\n",
            "BLlrsurovs s yeb s szaaluyerümYidoaukuu\n",
            "ö6lke y ryr Bja\n",
            "cLey ü pvüuillu\n",
            "3zen\n",
            "ekyu\n",
            "oyl erosön8saücotiÇ \n",
            "----\n",
            "iter 100, loss: 12.912867\n",
            "----\n",
            " a\n",
            "ük el ytmkaea\n",
            "o tkt aömleadere\n",
            "yuyia son iy maor\n",
            "ıaylrr\n",
            "Ko  OliYuvae t s \n",
            "Nzlsie\n",
            "h\n",
            "Kicdkplarlonndorualemmuaaiuoegeanmolates uri te elzleinedd\n",
            "sl sonnel\n",
            "slnBesanriknil ur kerknonar C yai\n",
            "la\n",
            "tİük\n",
            "Çysa \n",
            "----\n",
            "iter 200, loss: 12.534104\n",
            "----\n",
            " bvlrgyarnrsnynbegaun enriÜurk fae Bi\n",
            "paüiiildyntKocddürmotsetti sorecgayüsmo\n",
            "Yayko a\n",
            "jasdoÖki\n",
            "eyroüyenÖegdn  clu üol\n",
            "Yğ vosttma\n",
            "YaBgekmYak \n",
            "$aiü\n",
            "Yarükya\n",
            "6ertce\n",
            "fine\n",
            "Kanya neyuüakriu me t‘y Ctm5k naiai \n",
            "----\n",
            "iter 300, loss: 12.120041\n",
            "----\n",
            "  üiiunmda\n",
            "KaseSa venare\n",
            "Bas\n",
            "ciüsikde eEpyü\n",
            "Ban kimunia lar lan\n",
            "zir Yer vabirinkn gdoyirzsianin i\n",
            "Yg Beda Btyaü Vye\n",
            "Ylai ile knemidbisdadadsaaiin ba cir vk\n",
            "Yigdeuyaik kelteyegdirk van u  Keliu iir ban  \n",
            "----\n",
            "iter 400, loss: 11.718898\n",
            "----\n",
            " nseyürana cüvyim ne bd\n",
            "fea Seril kolen\n",
            "Ki simi mya Boönipei \n",
            "A\n",
            "AGoTsa ure vyk ulam\n",
            "Kkana fa völb sar  gnlir ettrdaeünür k\n",
            "A.lcul\n",
            "tira aflrsirenu mam Aoe vzr barurczmaniTmanerrigBz o\n",
            "cu\n",
            "Kar niyii sina\n",
            " \n",
            "----\n",
            "iter 500, loss: 11.309675\n",
            "----\n",
            " inaKgülin\n",
            "yuda dirdik\n",
            "Sara kieKia barilmak Bisi yalGescn viriz bae bniyosi yekmekiü\n",
            "Kisn sin kama vztmck Dir dan cir \n",
            "AeDa volgirildebirn a\n",
            "Oe$\n",
            "ba bas\n",
            "oa vilvenalan kireu yera akmkkgarir ki yostananii \n",
            "----\n",
            "iter 600, loss: 10.926164\n",
            "----\n",
            " yübgaltu buBcagoldarbemegirinü\n",
            "Yarbayoyü kanisi darin bur nan  ina baredmussi 1ayama balbaranenin\n",
            "Bit oamsdagda viğdünardanuzünse\n",
            "Yayeri bancö basisil\n",
            "Gan nuna ma in ver lasinkkabaaso?gaavaydi\n",
            "2ir Ba  \n",
            "----\n",
            "iter 700, loss: 10.532273\n",
            "----\n",
            " urdna bör\n",
            "Adi\n",
            "Öiİ bir nir Ayizu gur bilafsasinn enayem\n",
            "Ca mir in nlr  ni bin kapa b\n",
            "Gi!mcürenaviGcebü van dal vilbd  der Yizudaz sazuzü zGi Cir\n",
            "Aernduybik Bilge dir kira van kir\n",
            "hyeyada merdin bomizk\n",
            " \n",
            "----\n",
            "iter 800, loss: 10.129581\n",
            "----\n",
            " cmu\n",
            "Can yagirinuzi sal yez\n",
            "Alenmmedözesu delacte durk gu kilm vurtekun da ğirıa caye gazmr\n",
            "`kEda bmr bamere o \n",
            "Non\n",
            "Ccililgani badindi bollagi büsdC Biluraunauri böyğn okasi böfs taginizdey\n",
            "Akihu \n",
            "ürde \n",
            "----\n",
            "iter 900, loss: 9.796149\n",
            "----\n",
            " Aliy vinalüu malmau öcihdenda bayemmasia\n",
            "Avmrdeni oplMan Oa\n",
            "bavora bih baseAda var nanmayaomar Csmayuga tas‘n bilgina dar7 var\n",
            "Odulinda oJdirk bKrri uc Car\n",
            "ABihdan bir bagon dalirdana gir\n",
            "Aors voruüy  \n",
            "----\n",
            "iter 1000, loss: 9.542172\n",
            "----\n",
            " B yamar yal lel\n",
            "Tmmih dalay\n",
            "Yarüyal iz vcIsa yağa vırmegiu\n",
            "Bilas pür pyka  Oulosö yonk vir ba biş\n",
            "Acıhgdu k\n",
            "TaŞ virmeleT raT pda piht…ga kilasvan tyr oyer\n",
            "Vaşur dan\n",
            "Girazmamisyesvir börotagasolarda oü \n",
            "----\n",
            "iter 1100, loss: 9.280821\n",
            "----\n",
            " n vir\n",
            "Abihsınkin cırib dir Ba dan aa şGrö sıl Yala vür vir\n",
            "Acurkemaz kan pura kir dar yarde\n",
            "YeDe viza banarmesıh unyO ga\n",
            "Kerizması vmr\n",
            "Ayçlacandedi kir fazur\n",
            "Ccıh pira yPsınyan\n",
            "Sosen seDd\n",
            "buraa baymüm \n",
            "----\n",
            "iter 1200, loss: 9.037212\n",
            "----\n",
            " yanaö\n",
            "Agan tırin\n",
            "Agön yaymabınan ii Öürka ıkaküabirda bor\n",
            "Acıhulasir onünau ana omaz Sa Bura? kazı v?zsmrk.ne kın bark vazmekur oğai virtCnduna \n",
            "Assh oğan zzakin ayığyasıldebtSöuüadin dirdan valda bnl \n",
            "----\n",
            "iter 1300, loss: 8.700469\n",
            "----\n",
            " u delaz da bünar vinmalde KTrBa vakhnmayın nanmüe\n",
            "AyalSaran dıcmanınaleeya 5AkAYa boraC böğan bk0da a vız\n",
            "Akinda dör bnalün kılvarah il\n",
            "pla derlün ma daş oğona AAsecıla …ögirmasır gonaz da tağanyelon  \n",
            "----\n",
            "iter 1400, loss: 8.510489\n",
            "----\n",
            " rduraDu Süç\n",
            "Aökh gasi tayanınaninlr\n",
            "âeya na\n",
            "Acirciytirer ger\n",
            "Taredınmasta inan yebu darıBmatez birerı Öası kayan birderün bermuyariile\n",
            "Tar vır ya\n",
            "AAvar bahte iü kerindem olundde tizna tay\n",
            "esıhaiü ma7  \n",
            "----\n",
            "iter 1500, loss: 8.455695\n",
            "----\n",
            " p okKuYlülde gin\n",
            "Yeran yasv sğer büğaikü ağa mev\n",
            "Seök huske küraCau bür\n",
            "ASuhvinmeri Ö\n",
            "AŞTe bir\n",
            "Aeğün yaşoe\n",
            "Çazi meCen nivse gülersu olurerda\n",
            "ollazi oükı dan\n",
            "Öapıle di gesur oörbası gevey ğetar iy Gcr\n",
            " \n",
            "----\n",
            "iter 1600, loss: 8.444990\n",
            "----\n",
            " garis süldtmtu de vinne mete\n",
            "Haslaahmiksismin gabon\n",
            "A emeyen dedik diz\n",
            "Aesi pamtasdediysmesunüeniu maz Oires\n",
            "Aekira kapkirii  lulör\n",
            "Abarohüyun Tizmak faydi o nurman gayer\n",
            "Biüsenezi kip yoratün vizn ku \n",
            "----\n",
            "iter 1700, loss: 8.338824\n",
            "----\n",
            " ülininiyuz\n",
            "Bir\n",
            "Açcukiz ki gökove\n",
            "Aoşlagin\n",
            "Feyii\n",
            "ıDleri\n",
            "Yün\n",
            "danun k\n",
            "Sorismikin\n",
            "ge bikemesybnz oa\n",
            "Caversn luralce gasir gardacin\n",
            "Ça olki varka bireytulca dan gemekikafi ikll gede fisvüni dı olci Saglane \n",
            "----\n",
            "iter 1800, loss: 8.166363\n",
            "----\n",
            " \n",
            "Avorda durkizgini Hir\n",
            "Yasan kipi\n",
            "Kerim kiz eyak H\n",
            "Kirivin yeyosesmak firina yarmayloygu fi giekk Kırda keCirsirin ki kevorin veSgoruy hason kisin in savdekinlega\n",
            "'in\n",
            "Amorikizin\n",
            "Aacebik\n",
            "Sin kizligen g \n",
            "----\n",
            "iter 1900, loss: 8.009141\n",
            "----\n",
            " za\n",
            "Aopu mak b\n",
            "ğylcei\n",
            "Aöeda girolu masünza outem dez oyli kirha völmanegtü molesmisizie oykmau gev\n",
            "Yinimdim dem kiz in mi bantulen bir bele\n",
            "Han une balmy\n",
            "Ame\n",
            "Aaymak ineg\n",
            "Adur\n",
            "Sunuz evonmude gile\n",
            "Çeme b \n",
            "----\n",
            "iter 2000, loss: 7.921300\n",
            "----\n",
            " ndnge mafga bikmçman\n",
            "Amekgkz bövamci dirme fölul kizin vur oltemau \n",
            "Amlrmlez olesinde meykinin oyrin nlnl kizi deylelmis ğevötkmun ayendezg kirdila kerulingı gin singu geakih bakci gör damdir uümlel ö \n",
            "----\n",
            "iter 2100, loss: 7.748974\n",
            "----\n",
            " e oleti\n",
            "Sole bador oğvamaklise Bilaz\n",
            "Cavanvirtararizlin a buren ker\n",
            "Akarisgimez kizini sevöa vezalin siriniz\n",
            "Ade Birdo vilizmadilmör vyldinil Heriume hgyürr bakosiniki  ysakük pbini yacmane kör ekla\n",
            "S \n",
            "----\n",
            "iter 2200, loss: 7.759139\n",
            "----\n",
            " rin sasezaginkiz  oyanemasdezh yircizeünüz\n",
            "?ndi HTrküz uyekyytcda eürdurdekir h yluler\n",
            "Araner firag\n",
            "Klerin .lyilvlümenBirek\n",
            "Ezuzi yevin\n",
            "‘e yöu\n",
            "orpat voytümemaz Hekihis gekaz\n",
            "Huke kazin yirini müriz ba \n",
            "----\n",
            "iter 2300, loss: 7.610943\n",
            "----\n",
            " ert\n",
            "Azcuke sikhcik yerkaz mavalganik demc vökksisi gadekizi dimadnnmarin fine oulmnlovmek\n",
            "Acirlk bivktkcice\n",
            "Kari mutlulı Hlumer oollim Hakeyöa kiryesinifi k Derlik nendezgir\n",
            "Kisin Çcek$fi ciri meksogd \n",
            "----\n",
            "iter 2400, loss: 7.379322\n",
            "----\n",
            " yik fiy\n",
            "eveyav\n",
            "AGor kürus kişizi  cirecik\n",
            "Adör sinun n kerkü köki\n",
            "Aderlaren bevsin gurnamemler\n",
            "Hades bikm kiri sin yanebün\n",
            "Kinen okr serkür osgek di körnu uedek\n",
            "Cak otek nagi la gin\n",
            "pn yeleYccat yasın \n",
            "----\n",
            "iter 2500, loss: 7.239497\n",
            "----\n",
            "  takttylerigig Blvdeytun\n",
            "Cek sak oglglgi börnu Havtur Havun kezkizgi meynu da fiztin ilemcirey k\n",
            "Azi mertin\n",
            "Dankiscigdimokkük\n",
            "Adi Harger badge lekome vörkivi böksik birca damramis makaz söragan kir fö \n",
            "----\n",
            "iter 2600, loss: 7.073506\n",
            "----\n",
            " vdek\n",
            "Ta ver\n",
            "Asezdin gan hokur nür kizge\n",
            "Körkemegi mor gak\n",
            "Ve bamua okr saznemeb bin Hurciz mök kiztir firi muluu bur onmunkin larim berizme\n",
            "hulun bavvdr fikcö kizgamui elemeemeumür fir akramkizirnakea \n",
            "----\n",
            "iter 2700, loss: 6.961296\n",
            "----\n",
            " rin naz obyn kideg\n",
            "ğirizmize burgevgin a yösler aryla miruz delollun aleylur\n",
            "Hvkrmarlol kiyir payellolkafinKi Hakaniu\n",
            "HŞulek\n",
            "Tavin ganu lende demdamentimin eyegtamer haki\n",
            "KbamDrlamün vervasemvuler\n",
            "Dur \n",
            "----\n",
            "iter 2800, loss: 7.024757\n",
            "----\n",
            "  Aavenslekizvi bin küzbizılmek sorne cinunmekhalanu memag de\n",
            "Addn dazdemelmaükı ıla kizın Hardüz\n",
            "ısin yik yculecıSile elciledan soode fibun detin fildi\n",
            "Kan\n",
            "Aver\n",
            "hanzg guran hdrkda teku sirge yezlı\n",
            "i9d \n",
            "----\n",
            "iter 2900, loss: 7.027966\n",
            "----\n",
            " ır nrenrin fğrın Havdermazlı huna lınsamdu kizınrage kırin yyün k\n",
            "Avir furen ol kTinin fıI…çk\n",
            "Tiran navur oide bıava vurda geşvom glu fiSdak oglumafız gızı der met.imün\n",
            "Elen fıSilı \n",
            "Erşir gazıo\n",
            "O rulu \n",
            "----\n",
            "iter 3000, loss: 6.876007\n",
            "----\n",
            " in gevggl veu kir\n",
            "Adet vnagın firci Yur lirıztekıda vık\n",
            "Ctoklbi  erktrir kürik\n",
            "Adir fyakır\n",
            "Hügto  Havek\n",
            "Dtamekez dak\n",
            "ÖvululdemekHagık kimelvazıkızi mek Barlumu küz fir HYrmar kir ukan yür küze kizı eu \n",
            "----\n",
            "iter 3100, loss: 6.819737\n",
            "----\n",
            " emümlemde kiniyfulu meysa ağagizün\n",
            "Kin pna bülela da kimeelmuy bavan uluc\n",
            "Hsöde kule bemaltafın ağe de dır\n",
            "Çak\n",
            "Heryat olura mürcohum\n",
            "Havda bin an fal k\n",
            "Hiyrumtemon görmeh\n",
            "Ogrde föpcı  örvulgız fıği ma \n",
            "----\n",
            "iter 3200, loss: 6.819793\n",
            "----\n",
            " ermamefKakuk yarna kçza  ölvur k\n",
            "Ameva bay\n",
            "Aava vuzu kızg ysa ağlale kürıü in barciyer\n",
            "Adiz alele sönki gekızi mak\n",
            "Amağıgir Ci gik bılan uele yeulı dövgüz sırına küraimer birulu kıle bin der bavbaldın \n",
            "----\n",
            "iter 3300, loss: 6.886211\n",
            "----\n",
            " azmün glurik ele fır anuanin garıfı fircimınok la kızsık\n",
            "Amem kürız kakan yayen garkazli sanderın gerak il\n",
            "ude ka ba\n",
            "Aay kazgla ler\n",
            "Erız liz daretündemayAsde  iyve derıokü sipkerin fızga demamik\n",
            "Hapla \n",
            "----\n",
            "iter 3400, loss: 6.767337\n",
            "----\n",
            " ı merız dıkıntılın ağek fızdane kasili lu deven kelsakmakız larihniz sızın Hivk zömülsın Beze vırva kizdak fsroor kine kıngasız Hanz ohuyan ek berciYsum  oldu de dir gur oyluk fızı k\n",
            "Eçin uSdın de ba  \n",
            "----\n",
            "iter 3500, loss: 6.607261\n",
            "----\n",
            " rini kı güşi mir\n",
            "Trci de kızgızma oğl\n",
            "h biki bircek kırın Havim gabi savak\n",
            "Adem devgkmasıp Havsir avtazdemeylam,de kilglkm\n",
            "Isöneb kaşın det ,lu\n",
            "Kızı molem\n",
            "Epdi kılinsayına dı\n",
            "Ayavur yevy k\n",
            "Ler\n",
            "ole kim \n",
            "----\n",
            "iter 3600, loss: 6.527558\n",
            "----\n",
            " un yadimek berkün oğl k\n",
            "Yiran\n",
            "Havvnü kızı Hegil\n",
            "Çuzgımez fbükBu kızi kizı kişmererek mövan burolu keogan imanmasuE yakcime\n",
            "Odabagün Havva Huğüy ülur yavek\n",
            "Ade gelsıbim fir bel\n",
            "Kimın küleum bevsın kone \n",
            "----\n",
            "iter 3700, loss: 6.593531\n",
            "----\n",
            " en dek\n",
            "Kekuime heve barki bargır orutih burgi bamuluzg farCa\n",
            "tagle Öosbil\n",
            "Amdim anaurdu baün na f8üduümu den anani\n",
            "Ne vyül okuni serik\n",
            "GKurck\n",
            "Kiriri\n",
            "Yede\n",
            "Avurdetina\n",
            "BBaraHdele cezsige vena yöyu yerek  \n",
            "----\n",
            "iter 3800, loss: 6.751951\n",
            "----\n",
            " ben yani ecilerciyleri havdilusya kişisiriside\n",
            "Senye kızin banvman\n",
            "Ayşlez\n",
            "ozun de ki$e deri\n",
            "Oi gereki\n",
            "Becer bivdi mevule geu sen berarili bınk\n",
            "Amer bibik aokikonu avun bole hepulun boyem böcilemalsin  \n",
            "----\n",
            "iter 3900, loss: 6.783014\n",
            "----\n",
            " ik\n",
            "Dene\n",
            "Kituş ecelem kuşunu bon ugem bekmececin\n",
            "Kitinasigi decude ole ker ay bedem kebicaniur kankırmeugin yuri\n",
            "Serana sor meyame bim berdla fOk merman mur hyacs deziı zizim gik ogl\n",
            " ver besuluri bövk \n",
            "----\n",
            "iter 4000, loss: 6.781059\n",
            "----\n",
            " vcduur\n",
            "Yarar serini\n",
            "Koltcancilerilemim beyinek fucemikiş dem aybi deli\n",
            "bitar söyYegımi bidadiz kSne\n",
            "Köde me olusla kelöye be memibiz yerte yeyle\n",
            "Aderililisimurtilen öförtuiten lireyi\n",
            "?de\n",
            "Nkcinim dir c \n",
            "----\n",
            "iter 4100, loss: 6.717630\n",
            "----\n",
            " horcz\n",
            ",yıta\n",
            "Iolmin\n",
            "Adek\n",
            "Neyica gelmamerdünisi kur Ckanolaysenirisimas\n",
            "Edurat\n",
            "gan kcabsma veymura\n",
            "Adlat imemurmakerit oğa süktma ayula bir surtafBitın de\n",
            "me halbiler\n",
            "Gulu gua vet banca yanem kiyin \n",
            "Ade \n",
            "----\n",
            "iter 4200, loss: 6.811099\n",
            "----\n",
            " iki is her hakinma\n",
            "Öfalgı\n",
            "Saği\n",
            "Bmav Haahadasiz gice\n",
            "Hur baya\n",
            "Yerete\n",
            "'izı siC ofur banaş üyga vayec bon laşse\n",
            "ATfecük kez oğer mazsanur dürmelirar huu yürarim kariş yaz bakızin  ülur\n",
            "Ader hat kiran yok \n",
            "----\n",
            "iter 4300, loss: 6.825103\n",
            "----\n",
            " ta\n",
            "V?r demaşasi ola dar uluna obur yanan gadi gok da\n",
            "Sönde\n",
            "Afocut yen biyaylat ayana ayana decBuz derıri memek bönda kim kazeyan kakllusıracasin yirdimar yuyteşer lülu gaz eba suyda\n",
            "Ö: yağla kecaşde\n",
            "K \n",
            "----\n",
            "iter 4400, loss: 6.814206\n",
            "----\n",
            " lem barekmaz yap\n",
            "Ogla bige  oymi\n",
            "gorga du bedtu dij bede hünen anaum bapni de hu\n",
            "Gi manet im ğa vera çoğt beyatiri yen dim ağra oltan yazimer Sünatı iya sayBa vatiş kıriri gezbıda\n",
            "Çez in ekmem yenak o \n",
            "----\n",
            "iter 4500, loss: 6.898265\n",
            "----\n",
            " abo\n",
            "'un kek seyöşösi gaşdi eyla amele\n",
            "Foi i möbe kŞregı deşz\n",
            "BYllı de li\n",
            " bavur kizlem yellal\n",
            "İadisullmden kazsi meşsten buyle sirar menal a\n",
            "Gvt\n",
            "Abetzgan hay kanltu  onün yorli de dera  Avvime deşl\n",
            "Ad \n",
            "----\n",
            "iter 4600, loss: 6.899417\n",
            "----\n",
            "  buri\n",
            "Ki\n",
            "çaydi bıy beciğimele ba yanir bindi\n",
            "Betam getıde kek\n",
            "Kıltımdizlk cÖlan\n",
            "Adli\n",
            "Bel olti derllmüm sirem dezlicelli senye ale yek manla eyler gera\n",
            "\n",
            "Bal\n",
            "Name ger yereldi dort dem hilim k\n",
            "Sfram ok b \n",
            "----\n",
            "iter 4700, loss: 6.874103\n",
            "----\n",
            " soltubil\n",
            "imasiz yhllani\n",
            "Aböla mer çeluı yaşe heldimin keder damamemirimila bildis yerer k ya Oletızle karyimem birdip let\n",
            "Tati\n",
            "Öulik\n",
            "Keneleymemel eylema lagletzgl olli dini Caspsedtul hama  ofgip belg \n",
            "----\n",
            "iter 4800, loss: 6.954634\n",
            "----\n",
            " li buksi derolur\n",
            "Sok\n",
            "Cmekizla göstig de vellekli\n",
            "Sena\n",
            "otleyertasiIkahek oltasi otti semaş\n",
            "Kimuy didensisemmiteya kem dvevesi\n",
            "Çeremani dan Bğle molet\n",
            "Çulendi\n",
            "Neci aysayüali\n",
            "Huğum deme kemardetraTin död \n",
            "----\n",
            "iter 4900, loss: 7.030875\n",
            "----\n",
            " mtza bane bide saya\n",
            "Velvi\n",
            "Tasimam atdeka ke lem deldeli\n",
            "Keratar fşik sekabgeti deriz dermerdem dörez Sayelkuluf da eneldi velazk\n",
            "Katsa\n",
            "harimimd\n",
            "Sur ani casetile\n",
            "Hatı bun gasaşi ektüm tır or ciralyereh \n",
            "----\n",
            "iter 5000, loss: 7.020131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV9AV8TaXGpe"
      },
      "source": [
        "#GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu7MnF-tXJxl"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "import time\r\n",
        "import numpy as np"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBotwRABXOLT",
        "outputId": "40b739e1-6c5f-4cf5-fa06-2971ff2f16c3"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "textFileName = '/content/gdrive/My Drive/Colab Notebooks/Baris_Manco_lyricsText.txt'\r\n",
        "# Read, then decode for py2 compat.\r\n",
        "text = open(textFileName, 'rb').read().decode(encoding='utf-8')\r\n",
        "# length of text is the number of characters in it\r\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Length of text: 207287 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ytP-ZONXk2E",
        "outputId": "4fb1cbcd-68cb-4bfd-cd9b-23f810c43fff"
      },
      "source": [
        "# Take a look at the first 250 characters in text\r\n",
        "print(text[:250])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Denizlerde okyanuslarda\n",
            "Dalgalarin koynunda\n",
            "Bir ömür tükettikten sonra\n",
            "Su icip bogulmak varsa\n",
            "Cöllerde kizgin kumlarda\n",
            "Karli buzlu daglarda\n",
            "Bir ömür tükettikten sonra\n",
            "Cukurda kaybolmak varsa\n",
            "Korkunun ecele faydasi yok\n",
            "Bu kosusma niye\n",
            "Abbas yolcu sora\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdpoFjsFXo4l",
        "outputId": "3aff4635-6024-405f-91ec-f58c4237c564"
      },
      "source": [
        "# The unique characters in the file\r\n",
        "vocab = sorted(set(text))\r\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "81 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s51CLN66X8EV"
      },
      "source": [
        "#Metnin vektör hale getirilmesi\r\n",
        "İki tane sözlük(dictionary) oluşturuldu.\r\n",
        "Bir tanesi her bir karakterin int karşılığı diğeri ise her bir int değerin karakter karşılığını tutuyor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj5sJximYHay"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\r\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\r\n",
        "idx2char = np.array(vocab)\r\n",
        "\r\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7nhi-61YXq7",
        "outputId": "02bb9ded-4edf-4bc2-cc5c-8482e5f70995"
      },
      "source": [
        "print('{')\r\n",
        "for char,_ in zip(char2idx, range(20)):\r\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\r\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  \"'\" :   4,\n",
            "  ',' :   5,\n",
            "  '-' :   6,\n",
            "  '.' :   7,\n",
            "  '0' :   8,\n",
            "  '1' :   9,\n",
            "  '2' :  10,\n",
            "  '3' :  11,\n",
            "  '5' :  12,\n",
            "  '6' :  13,\n",
            "  '7' :  14,\n",
            "  '8' :  15,\n",
            "  '9' :  16,\n",
            "  ':' :  17,\n",
            "  '?' :  18,\n",
            "  'A' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1PBWMBhYm44"
      },
      "source": [
        "Artık her karakter için bir tamsayı temsiline sahip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VP-oxBxYZtm",
        "outputId": "a0772376-febd-4f16-c830-bbda025f5183"
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\r\n",
        "print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Denizlerde ok' ---- characters mapped to int ---- > [22 47 56 51 66 54 47 59 46 47  1 57 53]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apO6R-wmY94u"
      },
      "source": [
        "#Tahmin görevi\r\n",
        "Bir karakter veya karakter dizisi verildiğinde, en olası sonraki karakter nedir?\r\n",
        "\r\n",
        "Modelin girdisi bir karakter dizisi olacaktır ve modeli çıktıyı tahmin etmesi için eğitilecek.\r\n",
        "\r\n",
        "RNN'ler, bu ana kadar hesaplanan tüm karakterler göz önüne alındığında, daha önce görülen öğelere bağlı olan bir iç durumu koruduğundan, sonraki karakter nedir?\r\n",
        "\r\n",
        "Eğitim örnekleri ve hedefleri oluşturuldu.\r\n",
        "Ardından metni örnek dizilere bölündü. Her girdi dizisi metinden seq_length karakterleri içerecektir.\r\n",
        "\r\n",
        "Her giriş dizisi için, karşılık gelen hedefler, bir karakter sağa kaydırılması dışında aynı uzunlukta metin içerir.\r\n",
        "\r\n",
        "Bu yüzden metni seq_length + 1 parçalarına ayrıldı. Örneğin, seq_length 3 ve metnimizin \"Uzun\" ise, giriş dizisi \"Uzu\" ve hedef dizi \"zun\" olacaktır.\r\n",
        "\r\n",
        "Metin vektörünü bir karakter indeksleri akışına dönüştürmek için önce tf.data.Dataset.from_tensor_slices işlevi kullanıldı."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOQymN4LYqzi",
        "outputId": "7afaafd3-47f0-4950-d272-373c84356932"
      },
      "source": [
        "# The maximum length sentence you want for a single input in characters\r\n",
        "seq_length = 100\r\n",
        "examples_per_epoch = len(text)//(seq_length+1)\r\n",
        "\r\n",
        "# Create training examples / targets\r\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\r\n",
        "\r\n",
        "for i in char_dataset.take(5):\r\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "D\n",
            "e\n",
            "n\n",
            "i\n",
            "z\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz5rxz5ZZ_Gj"
      },
      "source": [
        "\"batch\" metodu, bu ayrı karakterleri kolayca istenen boyuttaki dizilere dönüştürmemizi sağlar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx9BoQ4QaHE5",
        "outputId": "7728068b-b2c7-4c47-cbb4-e776bfd760a3"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\r\n",
        "\r\n",
        "for item in sequences.take(5):\r\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Denizlerde okyanuslarda\\nDalgalarin koynunda\\nBir ömür tükettikten sonra\\nSu icip bogulmak varsa\\nCöllerd'\n",
            "'e kizgin kumlarda\\nKarli buzlu daglarda\\nBir ömür tükettikten sonra\\nCukurda kaybolmak varsa\\nKorkunun ec'\n",
            "'ele faydasi yok\\nBu kosusma niye\\nAbbas yolcu soran yok\\nYolculuk nereye\\nKim kaldi geriye\\nTas üstüne tas'\n",
            "' koya koya\\nYarattigin dünyanin\\nCöktügünü görmek bir yana\\nBirde altinda kalmak var ya\\nKorkunun ecele f'\n",
            "'aydsi yok\\nBu kosusma niye?\\nAbbas yolcu soran yok\\nYolculuk nereye\\nKim kaldi geriye\\nSana uzatilan eller'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjwruRzUazqY"
      },
      "source": [
        "Her bir diziye basit bir işlev uygulamak için \"batch\" metodu kullanılarak her sıra için çoğaltıldı ve girdi ve hedef metni oluşturmak üzere kaydırıldı."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdyv6Or8bB-x"
      },
      "source": [
        "def split_input_target(chunk):\r\n",
        "    input_text = chunk[:-1]\r\n",
        "    target_text = chunk[1:]\r\n",
        "    return input_text, target_text\r\n",
        "\r\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM7an0GTbDPg"
      },
      "source": [
        "İlk örnek girişi ve hedef değerleri"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-b4EqKfbKEQ",
        "outputId": "1245431e-f502-45f0-da8e-bfa5afaca369"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\r\n",
        "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\r\n",
        "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'Denizlerde okyanuslarda\\nDalgalarin koynunda\\nBir ömür tükettikten sonra\\nSu icip bogulmak varsa\\nCöller'\n",
            "Target data: 'enizlerde okyanuslarda\\nDalgalarin koynunda\\nBir ömür tükettikten sonra\\nSu icip bogulmak varsa\\nCöllerd'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uV0jo7SbV6l"
      },
      "source": [
        "Bu vektörlerin her bir indeksi, tek seferlik bir adım olarak işlenir. 0. zaman adımındaki girdi için, model \"F\" için indeksi alır ve sonraki karakter olarak \"i\" için indeksi tahmin etmeye çalışır. Bir sonraki zaman adımında, aynı şeyi yapar, ancak RNN, geçerli giriş karakterine ek olarak önceki adım içeriğini de dikkate alır."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWkHTr9JbW3h",
        "outputId": "ad5362f5-cc66-4c20-c228-e95d2dd154f4"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\r\n",
        "    print(\"Step {:4d}\".format(i))\r\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\r\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 22 ('D')\n",
            "  expected output: 47 ('e')\n",
            "Step    1\n",
            "  input: 47 ('e')\n",
            "  expected output: 56 ('n')\n",
            "Step    2\n",
            "  input: 56 ('n')\n",
            "  expected output: 51 ('i')\n",
            "Step    3\n",
            "  input: 51 ('i')\n",
            "  expected output: 66 ('z')\n",
            "Step    4\n",
            "  input: 66 ('z')\n",
            "  expected output: 54 ('l')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_QFjqVRbkgd"
      },
      "source": [
        "#Eğitim grupları oluşturulması\r\n",
        "Metni yönetilebilir dizilere bölmek için tf.data kullanıldı. \r\n",
        "\r\n",
        "Ancak bu verileri modele beslemeden önce, verileri karıştırmamız ve gruplar halinde paketlemeniz gerekir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h60TFLexbsdn",
        "outputId": "5f866a2c-a83c-447a-97f7-92441b344590"
      },
      "source": [
        "# Batch size\r\n",
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "# Buffer size to shuffle the dataset\r\n",
        "# (TF data is designed to work with possibly infinite sequences,\r\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\r\n",
        "# it maintains a buffer in which it shuffles elements).\r\n",
        "BUFFER_SIZE = 10000\r\n",
        "\r\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "\r\n",
        "dataset"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBotYpGTb2UF"
      },
      "source": [
        "#Model Oluşturulmaı\r\n",
        "Modeli tanımlamak için tf.keras.Sequential kullanıldı. Bu örnekte modeli tanımlamak için üç katman kullanılmıştır:\r\n",
        "\r\n",
        "tf.keras.layers.Embedding: Giriş katmanı. Her karakterin sayılarını embedding_dim boyutlarına sahip bir vektöre eşleyen eğitilebilir bir arama tablosu;\r\n",
        "\r\n",
        "tf.keras.layers.GRU: Boyut birimleri = rnn_units olan bir RNN türü \r\n",
        "\r\n",
        "tf.keras.layers.Dense: vocab_size çıktıları içeren çıktı katmanı."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LP6jm9JcTyV"
      },
      "source": [
        "# Length of the vocabulary in chars\r\n",
        "vocab_size = len(vocab)\r\n",
        "\r\n",
        "# The embedding dimension\r\n",
        "embedding_dim = 256\r\n",
        "\r\n",
        "# Number of RNN units\r\n",
        "rnn_units = 1024"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-_bf6x1cae4"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\r\n",
        "    model = tf.keras.Sequential([\r\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\r\n",
        "                                  batch_input_shape=[batch_size, None]),\r\n",
        "        tf.keras.layers.GRU(rnn_units,\r\n",
        "                            return_sequences=True,\r\n",
        "                            stateful=True,\r\n",
        "                            recurrent_initializer='glorot_uniform'),\r\n",
        "        tf.keras.layers.Dense(vocab_size)\r\n",
        "    ])\r\n",
        "    return model"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhCJpU2zchTK"
      },
      "source": [
        "model = build_model(\r\n",
        "    vocab_size=len(vocab),\r\n",
        "    embedding_dim=embedding_dim,\r\n",
        "    rnn_units=rnn_units,\r\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62PVOxvXcxxQ"
      },
      "source": [
        "Model her karakter için \"embedding\" arar, giriş olarak yerleştirme ile GRU'yu bir zaman adımı çalıştırır ve bir sonraki karakterin günlük olasılığını tahmin eden günlükleri oluşturmak için \"dense\" katmanı uygular:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po8rhEFVdF3J",
        "outputId": "da5197db-74e1-4a7a-ed09-d3c28d4bc6e9"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\r\n",
        "    example_batch_predictions = model(input_example_batch)\r\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 81) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSd-XsM5dk6q",
        "outputId": "0b6f1206-19dc-439c-a6b8-2eefe461c062"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_64\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_64 (Embedding)     (64, None, 256)           20736     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (64, None, 81)            83025     \n",
            "=================================================================\n",
            "Total params: 4,042,065\n",
            "Trainable params: 4,042,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBzNH1rPdy4M"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\r\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODxTI5vXew16",
        "outputId": "8b05b9c3-eab8-4c8b-cdf7-96c148c2b720"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([16, 61, 12, 14, 77, 13, 63, 23, 44, 27, 49, 45, 65, 30,  0,  4, 63,\n",
              "       38, 40, 62, 39, 34, 23, 10, 30, 65, 15, 70, 10, 11, 49, 39, 74, 11,\n",
              "       31, 61, 12,  6, 22, 69, 57, 38,  5,  6, 56,  0, 37,  3, 51, 62, 66,\n",
              "        5, 48, 67, 49, 26, 49,  4, 26, 65, 58, 78,  3, 43, 13, 50, 64, 68,\n",
              "       75, 54,  0, 50, 61, 32, 18, 68, 69,  8,  6, 26, 41, 74, 75, 34, 44,\n",
              "       27, 70, 23, 14, 69,  0, 28, 59, 17,  8, 10, 59, 42, 46, 30])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra1X1h_LexuK",
        "outputId": "a2123671-4f85-4ece-8393-01ae22d72ab9"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\r\n",
        "print()\r\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'da seni benden kim ayırabilir ki\\nÇocukça bir aşk deyipte geçme sakın gülme halime\\nNasıl oldu anlayam'\n",
            "\n",
            "Next Char Predictions: \n",
            " \"9t57Ş6vEbIgcyL\\n'vUYuVPE2Ly8â23gVğ3Mt5-DÜoU,-n\\nT$iuz,fÇgHg'Hypş$a6hwÖİl\\nhtN?ÖÜ0-HZğİPbIâE7Ü\\nJr:02r`dL\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdfDA9EejPOi"
      },
      "source": [
        "Modelin eğitilmesi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y00DxtN8i1Ti",
        "outputId": "107301e7-8f1d-432b-8a75-d1cabbb05b91"
      },
      "source": [
        "def loss(labels, logits):\r\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\r\n",
        "\r\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\r\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\r\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 81)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.394629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHzTlok8ja9H"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss, metrics = ['accuracy'])"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qro1xtJAje5E"
      },
      "source": [
        "# Directory where the checkpoints will be saved\r\n",
        "checkpoint_dir = './training_checkpoints'\r\n",
        "# Name of the checkpoint files\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n",
        "\r\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_prefix,\r\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnHx-ueVjihG"
      },
      "source": [
        "EPOCHS = 30"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05B45GEEjjBV",
        "outputId": "7a175a39-1050-4743-8baa-79e06e77c21d"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "32/32 [==============================] - 6s 130ms/step - loss: 4.1854 - accuracy: 0.1138\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 2.8997 - accuracy: 0.2114\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 2.4262 - accuracy: 0.2723\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 2.2958 - accuracy: 0.2893\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 5s 127ms/step - loss: 2.2120 - accuracy: 0.3096\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 5s 126ms/step - loss: 2.1445 - accuracy: 0.3255\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 2.0909 - accuracy: 0.3402\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 2.0236 - accuracy: 0.3637\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 1.9588 - accuracy: 0.3835\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 1.8867 - accuracy: 0.4117\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 1.7956 - accuracy: 0.4429\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 1.6934 - accuracy: 0.4787\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 1.5810 - accuracy: 0.5172\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 1.4610 - accuracy: 0.5605\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 1.3136 - accuracy: 0.6114\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 1.1754 - accuracy: 0.6597\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 1.0416 - accuracy: 0.7063\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.9031 - accuracy: 0.7541\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.7742 - accuracy: 0.7980\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.6592 - accuracy: 0.8378\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.5637 - accuracy: 0.8702\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.4879 - accuracy: 0.8936\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.4263 - accuracy: 0.9126\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.3759 - accuracy: 0.9257\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 0.3467 - accuracy: 0.9330\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.3140 - accuracy: 0.9394\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.2982 - accuracy: 0.9429\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.2817 - accuracy: 0.9460\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.2698 - accuracy: 0.9476\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.2577 - accuracy: 0.9489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "NO8rKYHIjItX",
        "outputId": "55d30e21-ce65-4a56-ad16-c6ddb74a8106"
      },
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "# Plot training & validation accuracy values\r\n",
        "plt.figure(figsize=(14,3))\r\n",
        "\r\n",
        "# Plot training & validation loss values\r\n",
        "plt.subplot(1, 2, 2)\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.title('Model loss')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.legend(['loss', 'Acc'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Emdlvf6ajyP3",
        "outputId": "2c39972c-450d-49fa-f031-6581568ee106"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_30'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC04VxGHjzMr"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\r\n",
        "\r\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\r\n",
        "\r\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxPmQiZfj25a",
        "outputId": "700d1034-cc05-4f3b-bbea-be5a79ff8b04"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_65\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_65 (Embedding)     (1, None, 256)            20736     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (1, None, 81)             83025     \n",
            "=================================================================\n",
            "Total params: 4,042,065\n",
            "Trainable params: 4,042,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX9RQ9c1kGga"
      },
      "source": [
        "Prediction loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svp4eCh9kI2E"
      },
      "source": [
        "def generate_text(model, start_string):\r\n",
        "    # Evaluation step (generating text using the learned model)\r\n",
        "\r\n",
        "    # Number of characters to generate\r\n",
        "    num_generate = 500\r\n",
        "\r\n",
        "    # Converting our start string to numbers (vectorizing)\r\n",
        "    input_eval = [char2idx[s] for s in start_string]\r\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\r\n",
        "\r\n",
        "    # Empty string to store our results\r\n",
        "    text_generated = []\r\n",
        "\r\n",
        "    # Low temperature results in more predictable text.\r\n",
        "    # Higher temperature results in more surprising text.\r\n",
        "    # Experiment to find the best setting.\r\n",
        "    temperature = 1.0\r\n",
        "\r\n",
        "    # Here batch size == 1\r\n",
        "    model.reset_states()\r\n",
        "    for i in range(num_generate):\r\n",
        "        predictions = model(input_eval)\r\n",
        "        # remove the batch dimension\r\n",
        "        predictions = tf.squeeze(predictions, 0)\r\n",
        "\r\n",
        "        # using a categorical distribution to predict the character returned by the model\r\n",
        "        predictions = predictions / temperature\r\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\r\n",
        "\r\n",
        "        # Pass the predicted character as the next input to the model\r\n",
        "        # along with the previous hidden state\r\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\r\n",
        "\r\n",
        "        text_generated.append(idx2char[predicted_id])\r\n",
        "\r\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuEjrGQrkNAx",
        "outputId": "876b2bc8-84b2-4431-cf06-35fc9425b542"
      },
      "source": [
        "print(generate_text(model, start_string=u\"Dağlar: \"))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dağlar: şemilitine sen unda yanıyor yüreğim\n",
            "Sabırı de dayıyor murum bu yok\n",
            "Senin için dinlere düştüm derdime dermanım nerde\n",
            "Hel Ahmet Bey‘in Ceketi\n",
            "Sonunda herkes anladi ya nasip ne de yatmış\n",
            "Yıla ruslan gibi kaçak sevdedi didi ikiti\n",
            "I bir aği bahalım yüterim ben bilirim bahça o8n\n",
            "Bağım bir tane\n",
            "Çıt çıt çıt çıt çedene de sar bedeni bedene\n",
            "Dünya dolu yar olsa da olur\n",
            "\n",
            "Odum baga ver biraz da ona Diliyorum\n",
            "Ne oldu bana böyle durup dururken\n",
            "Elemim sabbir nca\n",
            "Bir de şarim sefdiğim olsun biriz yaynan brem var\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV2d3PyQfhvC"
      },
      "source": [
        "#LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT7tT8l9fk4f",
        "outputId": "05b03352-96d5-4dc1-f0b4-51db06b80e6f"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "textFileName = '/content/gdrive/My Drive/Colab Notebooks/Baris_Manco_lyricsText.txt'\r\n",
        "\r\n",
        "def process_text(file_path):\r\n",
        "    text = open(file_path, 'rb').read().decode(encoding='utf-8')  # Read, then decode for py2 compat.\r\n",
        "    vocab = sorted(set(text))  # The unique characters in the file\r\n",
        "    # Creating a mapping from unique characters to indices and vice versa\r\n",
        "    char2idx = {u: i for i, u in enumerate(vocab)}\r\n",
        "    idx2char = np.array(vocab)\r\n",
        "    text_as_int = np.array([char2idx[c] for c in text])\r\n",
        "    return text_as_int, vocab, char2idx, idx2char\r\n",
        "\r\n",
        "\r\n",
        "def split_input_target(chunk):\r\n",
        "    input_text, target_text = chunk[:-1], chunk[1:]\r\n",
        "    return input_text, target_text\r\n",
        "\r\n",
        "def create_dataset(text_as_int, seq_length=100, batch_size=64, buffer_size=10000):\r\n",
        "    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\r\n",
        "    dataset = char_dataset.batch(seq_length + 1, drop_remainder=True).map(split_input_target)\r\n",
        "    dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\r\n",
        "    return dataset\r\n",
        "\r\n",
        "def build_model(vocab_size, embedding_dim=256, rnn_units=1024, batch_size=64):\r\n",
        "    model = tf.keras.Sequential([\r\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\r\n",
        "        tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\r\n",
        "        #tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\r\n",
        "        tf.keras.layers.Dense(vocab_size)\r\n",
        "    ])\r\n",
        "    return model\r\n",
        "\r\n",
        "def loss(labels, logits):\r\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\r\n",
        "\r\n",
        "def generate_text(model, char2idx, idx2char, start_string, generate_char_num=1000, temperature=1.0):\r\n",
        "    # Evaluation step (generating text using the learned model)\r\n",
        "    # Low temperatures results in more predictable text, higher temperatures results in more surprising text.\r\n",
        "    # Converting our start string to numbers (vectorizing)\r\n",
        "    input_eval = [char2idx[s] for s in start_string]\r\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\r\n",
        "    text_generated = []  # Empty string to store our results\r\n",
        "    model.reset_states()\r\n",
        "    for i in range(generate_char_num):\r\n",
        "        predictions = model(input_eval)\r\n",
        "        predictions = tf.squeeze(predictions, 0)    # remove the batch dimension\r\n",
        "        predictions /= temperature\r\n",
        "        # using a categorical distribution to predict the character returned by the model\r\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\r\n",
        "        # We pass the predicted character as the next input to the model along with the previous hidden state\r\n",
        "        input_eval = tf.expand_dims([predicted_id], axis=0)\r\n",
        "        text_generated.append(idx2char[predicted_id])\r\n",
        "    return start_string + ''.join(text_generated)\r\n",
        "\r\n",
        "path_to_file = textFileName\r\n",
        "\r\n",
        "text_as_int, vocab, char2idx, idx2char = process_text(path_to_file)\r\n",
        "\r\n",
        "dataset = create_dataset(text_as_int)\r\n",
        "model = build_model(vocab_size=len(vocab))\r\n",
        "model.compile(optimizer='adam', loss=loss, metrics = ['accuracy'])\r\n",
        "model.summary()\r\n",
        "\r\n",
        "history = model.fit(dataset, epochs=30)\r\n",
        "\r\n",
        "model.save_weights(\"gen_text_weights.h5\", save_format='h5')\r\n",
        "# To keep this prediction step simple, use a batch size of 1\r\n",
        "model = build_model(vocab_size=len(vocab), batch_size=1)\r\n",
        "model.load_weights(\"gen_text_weights.h5\")\r\n",
        "model.summary()\r\n",
        "\r\n",
        "#user_input = input(\"Write the beginning of the text, the program will complete it. Your input is: \")\r\n",
        "generated_text = generate_text(model, char2idx, idx2char, start_string=u\"Dağlar: \", generate_char_num=1000)\r\n",
        "print(generated_text)\r\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "Model: \"sequential_66\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_66 (Embedding)     (64, None, 256)           20736     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (64, None, 81)            83025     \n",
            "=================================================================\n",
            "Total params: 4,042,065\n",
            "Trainable params: 4,042,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 6s 132ms/step - loss: 3.9920 - accuracy: 0.1098\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 2.8721 - accuracy: 0.2120\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 2.4142 - accuracy: 0.2726\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 2.2875 - accuracy: 0.2914\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 2.2077 - accuracy: 0.3103\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 2.1400 - accuracy: 0.3256\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 2.0871 - accuracy: 0.3424\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 2.0238 - accuracy: 0.3611\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 1.9487 - accuracy: 0.3873\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 1.8690 - accuracy: 0.4141\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 1.7847 - accuracy: 0.4448\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 1.6764 - accuracy: 0.4835\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 1.5603 - accuracy: 0.5236\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 1.4209 - accuracy: 0.5743\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 1.2795 - accuracy: 0.6214\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 1.1386 - accuracy: 0.6715\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.9965 - accuracy: 0.7203\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.8491 - accuracy: 0.7718\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.7296 - accuracy: 0.8127\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 0.6209 - accuracy: 0.8493\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.5192 - accuracy: 0.8826\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.4577 - accuracy: 0.9037\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.3916 - accuracy: 0.9211\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.3542 - accuracy: 0.9304\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 4s 128ms/step - loss: 0.3255 - accuracy: 0.9368\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.3065 - accuracy: 0.9405\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.2854 - accuracy: 0.9446\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.2691 - accuracy: 0.9466\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.2621 - accuracy: 0.9479\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.2523 - accuracy: 0.9496\n",
            "Model: \"sequential_67\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_67 (Embedding)     (1, None, 256)            20736     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (1, None, 81)             83025     \n",
            "=================================================================\n",
            "Total params: 4,042,065\n",
            "Trainable params: 4,042,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Dağlar: hele hele yar yar\n",
            "Tuna boyun da baga vir\n",
            "Ayrana daldir bulgura saldir\n",
            "Selam verdin geline\n",
            "Kara kılmaz ve hatta Hint okyanusu\n",
            "Ve hanuna yoğdan oy yüreğime al beni kollarına\n",
            "Gurmet sarı çimez solbuş gece vakti dellenme Okman\n",
            "Evek banı al gili Mehm Osman\n",
            "Bak hader verdin.\n",
            "Daglar daglar...\n",
            "Kırk yıl bir yastıkta tam kırk yıl\n",
            "Anamda bayanelerdikinde ne ola\n",
            "Göç eden kuşlar gibi gel\n",
            "Zatile diylerim dayan\n",
            "Emmim atışlı dağlardan a böyle kurulmus\n",
            "Dünya böyle kurulmus\n",
            "Her ademogluna bir havva nasip olmus\n",
            "Yeter içecek beni ayrılıkları\n",
            "Senin içinlerdun ay Osman\n",
            "Çabak iler yanımda\n",
            "Heremediyle ayrılırken\n",
            "Herkes duy sekelem\n",
            "Gözlerimde sanciki düşman sıradım sormamayın bremişt kapan ılıp gitmiş balgsur\n",
            "Karacek bir gacebiğin benim garipleri gibi\n",
            "\n",
            "Tabran komşu kız ı gayri oy\n",
            "Kelp mi söyle öle yüz mı?\n",
            "Vur ha kardaş vur ha vur ha vur\n",
            "Yarıler yarimde mana sor\n",
            "Nasıl kıydın kendine gücelem yanıyor yüreğim\n",
            "Yine gözyaşlarım yağmur gibi\n",
            "Yaşıyorum'a östü düşlam demedim ben\n",
            "Hayırsızı kitapsızı zalimi lahmacun\n",
            "Mah e\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}